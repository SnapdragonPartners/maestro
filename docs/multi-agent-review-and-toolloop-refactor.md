
# Multi-Agent Go System Review & Refactor Plan

This document summarizes an architectural/code-quality review of the three agents (`arch`, `coder`, `pm`) plus the shared `toolloop` package, and proposes a concrete refactor plan.

The emphasis is on:

- DRYness and clarity of responsibilities
- Removing dead code and debug cruft
- Consistent, robust behavior aligned with Go idioms
- A **clean break** for `toolloop` with compiler-enforced migration
- Aligning with your operational constraints:
  - Agents run as unattended as possible
  - Failure is expensive; human escalation is preferred over failing
  - Token budgets and rate limits are enforced elsewhere (LLM middleware)
  - Iteration limits here are about preventing infinite loops and enforcing ‚Äúmeaningful progress‚Äù

---

## 1. How the three agents differ architecturally

### 1.1 Common foundation

Across all three agents, there is a clear shared foundation:

- All embed `*agent.BaseStateMachine` and use it as the single source of truth for state and state data.
- All use a consistent `ProcessState` / `executeState` / `processCurrentState` pattern with a `switch` over `GetCurrentState()`, returning `(nextState, done, err)`.
- All use the same pattern of **effects + runtime** (`effect.BaseRuntime`, `ExecuteEffect`, ‚Äúrequest/result‚Äù agent messages) to interact with the dispatcher/chat service.
- All use **toolloop** with type-parameterized `Config[T]` for LLM tooling, plus `CheckTerminal` and `ExtractResult` functions.

So the migration to a unified state machine + tool-based LLM interaction is real and coherent.

---

### 1.2 Architect agent

Architect is the **approval and review hub**:

- It doesn‚Äôt own a long-running ‚Äústory workspace‚Äù; instead it dynamically creates read-only tool providers pointed into coder workspaces (e.g. `createReadToolProviderForCoder`).
- It has two distinct flavors of LLM interactions:
  - **Iterative approvals** (code, completion) with rich exploration via tools rooted at `/mnt/coders/<coder>` workspaces.
  - **Single-turn reviews** (plans, budget reviews) using dedicated tools like `review_complete`, enforced with `SingleTurn: true` and small `MaxIterations`.
- It owns **spec review and question answering flows**:
  - `generatePlanPrompt` / `generateBudgetPrompt` / `generateQuestionPrompt`, each tightly coupled to specific tool sets and templates, with ‚Äúyou MUST call <tool>`‚Äù instructions.
  - Tool extractors like `ExtractSubmitReply`, `ExtractSpecReview`, `ExtractReviewComplete` that decode tool results into typed structs.
- It has its own **iteration-based escalation** for LLM usage (`EscalationConfig` in toolloop), plus explicit state keys for escalation tracking, e.g. `StateKeyEscalationRequestID`, `StateKeyEscalationStoryID`, and `StateKeyPattern*` iteration keys.

Conceptually: Architect is a **service-style** agent that reacts to incoming requests, runs bounded tool-based exploration, and returns typed approval/answer messages‚Äîmostly stateless between stories except for escalation tracking and some metrics.

Operationally:

- Architect is a singleton serving many coders.
- Architect context is intentionally short-lived (per-state, per-request).
- Architect failures are currently treated as fatal for the overall app, but the long-term plan is to **escalate to a human (possibly via the PM)**.

---

### 1.3 Coder agent

Coder is **stateful and lifecycle-heavy**:

- It owns the whole **implementation lifecycle**:

  ```text
  WAITING ‚Üí SETUP ‚Üí PLANNING ‚Üí PLAN_REVIEW ‚Üí CODING ‚Üí TESTING ‚Üí CODE_REVIEW ‚Üí PREPARE_MERGE ‚Üí AWAIT_MERGE ‚Üí DONE/ERROR
  ```

  Plus `BUDGET_REVIEW` and `QUESTION`.

- It manages real resources:
  - Workspaces and filesystems
  - Containers and build backends (`build.Service`, `LongRunningDockerExec`)
  - Git branches and PRs via `CloneManager`
- It has its own internal **todo list abstraction** (`TodoList`, `TodoItem`, add/update/complete flows) and a separate todo-collection toolloop phase.
- It implements a **budget escalation path**:
  - Planning and coding phases maintain iteration counts and, on hitting configured limits, transition into `BUDGET_REVIEW` by storing a `BudgetReviewEffect` in state and then executing it, which sends a typed approval request to the architect.
  - `handleBudgetReview` executes this effect and then processes `BudgetReviewResult` to decide whether to return to `PLANNING`, `CODING`, or `ERROR`.
- It has rich testing behavior:
  - Distinct ‚ÄúDevOps‚Äù vs ‚ÄúApp‚Äù testing strategies (`handleDevOpsStoryTesting`, `handleAppStoryTesting`, `handleLegacyTesting`), using either `build.Service` or direct `make test` via `LongRunningDockerExec`.

Conceptually: Coder is the **long-lived worker** with a deep, multi-stage FSM and a lot of operational responsibilities. It‚Äôs also where the most complexity and entropy has accumulated.

Operationally:

- Many coders can exist concurrently (target ~10).
- Each coder handles exactly one story and is torn down after completion; context, clones, and workspaces are deleted to guarantee a clean slate next time.
- If a coder fails during story development, the story is re-queued for another attempt.
- IDs (agent ID, story ID, workspace paths) are essential for tying everything together; they must not be duplicated inconsistently, but they are intentionally abundant.

---

### 1.4 PM agent

PM is **requirements- and spec-centric**, closer to a ‚Äúfront-of-house‚Äù agent:

- Its FSM is narrower, focused on story intake, requirement clarification, spec drafting, and hand-off:
  - States like `WAITING`, `INTERVIEW`, `SPEC_DRAFTING`, `SPEC_REVIEW`, `PREVIEW`, `AWAIT_USER`, `AWAIT_ARCHITECT`, etc.
- It uses toolloop primarily to:
  - Drive **spec creation and refinement** via tools like `spec_submit`, not filesystem/build tools.
- Its escalation path is **procedural rather than budget-driven**:
  - It doesn‚Äôt maintain complex iteration counters; escalation is about ‚Äúspec needs architect review‚Äù or ‚Äúwaiting for user/architect response‚Äù.
  - The PM is the **only agent that directly interacts with a human user**; escalation typically means using `chat_ask_user` to provide a status update and ask for permission to continue/revise.
- It still uses the same `BaseRuntime + SendMessageEffect` pattern to talk to the dispatcher and receive replies.

Conceptually: PM is the **requirements orchestrator**‚Äîmore about conversational/spec flows and less about long-running tooling or changes to the filesystem.

Operationally:

- PM is the preferred escalation surface for human involvement when possible.
- Escalation to a human is preferred over failure, but both are undesirable; the system is intended to run unattended most of the time.

---

## 2. Cross-cutting, high-priority improvements

This section summarizes high-impact changes across agents and shared infrastructure (including the updated understanding around `toolloop` and escalation).

### 2.1 DRY: unify the Runtime + SendMessageEffect pattern

Right now each agent has its own small runtime wrapper:

- `type Runtime struct { *effect.BaseRuntime; <agent> *<Agent> }`
- A `NewRuntime` that simply calls `effect.NewBaseRuntime(dispatcher, logger, agentID, "<agentType>")`.

The logic is effectively identical except for the agent type string and maybe one or two helper methods.

**Proposal**

Introduce a single reusable runtime type in the `effect` package, e.g.:

```go
type AgentRuntime struct {
    *BaseRuntime
    agentType agent.Type
}

func NewAgentRuntime(dispatcher Dispatcher, logger logx.Logger, id string, t agent.Type) *AgentRuntime {
    br := NewBaseRuntime(dispatcher, logger, id, t)
    return &AgentRuntime{BaseRuntime: br, agentType: t}
}
```

Each agent then embeds or holds `*effect.AgentRuntime` instead of duplicating boilerplate. Agent-specific helpers (like `SendMessageEffect` wrappers) can be small methods on their own agent struct instead of re-declaring full runtime types.

**Benefit**

- Removes near-duplicate runtime definitions and any risk of subtle divergence (e.g. logging fields, metrics tags).
- Makes it trivial to change shared runtime behavior (e.g., standardized result envelopes, tracing) in one place.

---

### 2.2 DRY: centralize ‚Äústate handler switch + error semantics‚Äù

All agents follow the same general pattern:

- Read `current := sm.GetCurrentState()`.
- `switch current` and call `handleX` for each state.
- If the handler returns an error, log it, set an error message in state, and transition to `StateError` (or equivalent).

The implementations are similar but not identical, which invites unintentional differences in:

- What gets logged
- How error messages are stored
- Whether `StateDone` requires special handling

**Proposal**

Introduce a small helper in the `agent` package (or embed in `BaseStateMachine`) such as:

```go
type StateHandler func(ctx context.Context, sm *BaseStateMachine) (next proto.State, done bool, err error)

func RunStep(
    ctx context.Context,
    sm *BaseStateMachine,
    logger logx.Logger,
    handlers map[proto.State]StateHandler,
) (proto.State, bool, error) {
    cur := sm.GetCurrentState()
    handler, ok := handlers[cur]
    if !ok {
        err := fmt.Errorf("no handler for state %s", cur)
        sm.SetErrorMessage(err.Error())
        logger.Error("unhandled state", "state", cur, "err", err)
        return proto.StateError, true, err
    }

    next, done, err := handler(ctx, sm)
    if err != nil {
        sm.SetErrorMessage(err.Error())
        logger.Error("state handler error", "state", cur, "err", err)
        return proto.StateError, true, err
    }

    sm.SetCurrentState(next)
    return next, done, nil
}
```

Each agent then:

- Registers its `handlers` map once (e.g., in constructor), and
- Calls `RunStep` from its state-processing loop.

**Benefit**

- Single source of truth for error handling semantics.
- Simplifies adding cross-cutting behavior (metrics on state transitions, tracing) uniformly across agents.

---

### 2.3 DRY + correctness: standardize escalation patterns

Escalation is conceptually similar across agents but implemented differently:

- **Architect**: iteration-based escalation per story/request, stored in state keys, with `EscalationConfig` hooks.
- **Coder**: iteration ‚Üí budget review; stores a `BudgetReviewEffect` and transitions to `BUDGET_REVIEW`.
- **PM**: escalation is human-centric (await user, ask for input); it‚Äôs not iteration-budget-driven.

Given your clarified goals:

- System should operate unattended whenever possible.
- Failure is expensive; when stuck, **escalate to a human** via PM or Architect as appropriate.
- Only the PM talks directly to humans (via tools like `chat_ask_user`).

**Proposal**

Rather than forcing identical escalation behavior, define a small vocabulary + helper that each agent can use:

```go
type EscalationKind string

const (
    EscalationKindLLMBudget        EscalationKind = "llm_budget"
    EscalationKindHumanClarify     EscalationKind = "human_clarify"
    EscalationKindPlanApproval     EscalationKind = "plan_approval"
    EscalationKindBudgetReview     EscalationKind = "budget_review"
    EscalationKindStoryRequeue     EscalationKind = "story_requeue"
    // etc.
)

func Escalate(sm *agent.BaseStateMachine, kind EscalationKind, metadata map[string]any) {
    // store in state in a consistent format, or emit a standardized effect
}
```

Use this from:

- **Architect**: in `EscalationConfig.OnHardLimit` or ‚Äúno tools used twice‚Äù to record escalation intent and drive PM or user interaction.
- **Coder**: when iteration limits are hit ‚Üí `EscalationKindBudgetReview`, plus a `BudgetReviewEffect`.
- **PM**: when stuck or hitting iteration limits ‚Üí `EscalationKindHumanClarify`, plus `chat_ask_user` tool invocation.

**Benefit**

- Provides consistent, queryable escalation semantics for metrics, observability, and UI.
- Reduces scattered magic strings and per-agent conventions.

---

### 2.4 DRY: unify toolloop usage around a typed Outcome

Originally, each agent used `toolloop.Run` with this kind of contract:

```go
signal, result, err := toolloop.Run(loop, ctx, cfg)
```

With different call sites manually interpreting:

- `IterationLimitError` for iteration limit escalation.
- Generic errors (including ‚Äúno tools used after reminder‚Äù).
- Empty `signal` vs ‚Äúterminal‚Äù signal names.

With the updated understanding:

- Token budgets and rate limits are handled elsewhere (LLM middleware).
- Iteration limits here are **safety rails** to avoid infinite loops or non-progress.
- Human escalation is strongly preferred over failing whenever we‚Äôre stuck.

The suggested clean-break refactor is:

#### 2.4.1 Make `toolloop.Run` return a structured `Outcome`

Define:

```go
type OutcomeKind int

const (
    OutcomeSuccess OutcomeKind = iota
    OutcomeNoToolTwice      // LLM ignored tools 2 turns in a row (even after reminder)
    OutcomeIterationLimit   // Escalation.HardLimit reached
    OutcomeMaxIterations    // MaxIterations hit without Escalation.HardLimit
    OutcomeLLMError         // LLM client failed (network, 5xx, context errors, etc.)
    OutcomeExtractionError  // CheckTerminal signaled, but ExtractResult failed
)

type Outcome[T any] struct {
    Kind      OutcomeKind
    Signal    string // whatever CheckTerminal returned, e.g. "SUBMIT_REPLY"
    Value     T      // valid when Kind == OutcomeSuccess
    Err       error  // underlying error, where relevant
    Iteration int    // last iteration index (useful for logs/metrics)
}
```

Change the signature to:

```go
func Run[T any](tl *ToolLoop, ctx context.Context, cfg *Config[T]) Outcome[T]
```

Inside `Run`, you already know when:

- No tools were used twice (after injecting a ‚Äúplease use tools‚Äù reminder).
- `Escalation.HardLimit` fired (today you return `*IterationLimitError`).
- `MaxIterations` was exceeded.
- The LLM call itself failed.
- Extraction failed.

Map these events to corresponding `OutcomeKind` and populate `Outcome[T]` accordingly.

This is a deliberate **breaking change** that forces every call site to be reviewed and updated; the compiler will help ensure nothing is missed.

#### 2.4.2 Typed sentinel errors (optional, for internal clarity)

Internally, you can keep / introduce a couple of typed errors to keep `Run` implementation clean:

```go
var ErrNoToolUsageTwice = errors.New("no tools used after reminder")

type IterationLimitError struct {
    Iteration int
    msg       string
}

func (e *IterationLimitError) Error() string { return e.msg }
```

`Run` can use these to decide which `OutcomeKind` to produce, but callers don‚Äôt need to do `errors.As` anymore ‚Äì they switch on `Outcome.Kind`.

#### 2.4.3 Agents interpret `Outcome` with explicit policies

Each agent‚Äôs call site now looks like this (example: Coder coding loop):

```go
out := toolloop.Run[CodingResult](loop, ctx, cfg)

switch out.Kind {
case toolloop.OutcomeSuccess:
    signal := out.Signal
    result := out.Value
    // normal handling based on signal/result

case toolloop.OutcomeIterationLimit:
    // For Coder: transition to BUDGET_REVIEW
    c.logger.Info("Coding iteration limit reached (%d), moving to BUDGET_REVIEW", out.Iteration)
    return StateBudgetReview, false, nil

case toolloop.OutcomeNoToolTwice:
    // Decide if this is a fatal error or a reason to escalate via PM/Architect
    return proto.StateError, false, logx.Wrap(out.Err, "coding LLM did not use tools")

case toolloop.OutcomeLLMError, toolloop.OutcomeMaxIterations, toolloop.OutcomeExtractionError:
    // Reuse your existing error handling (e.g., handleEmptyResponseError, requeue, etc.)
    return proto.StateError, false, logx.Wrap(out.Err, "toolloop execution failed")
}
```

Key point:

- **toolloop remains generic**; it doesn‚Äôt decide whether to budget review, ask user, or fail.
- Each agent encodes its own policy in a clear, compiler-checked switch.

This replaces the original ‚Äútoolloop runner + escalation‚Äù idea with a simpler, more Go-ish approach: a typed result plus explicit per-agent switch logic.

---

### 2.5 Unify ‚Äúno terminal tool‚Äù semantics via extractor contracts

Right now different extractors interpret ‚Äúno terminal tool was called‚Äù differently:

- Planning extractors may return error if no terminal tool is seen.
- Coding extractors sometimes treat ‚Äúpartial activity but no terminal tool‚Äù as success, and ‚Äúno activity‚Äù as error.
- Architect extractors (`ExtractSubmitReply`, `ExtractSpecReview`, etc.) generally treat missing required tools as errors.

In a tool-only interaction world, ‚Äúno terminal tool‚Äù and ‚Äúno meaningful activity‚Äù are central failure modes and should be treated consistently.

**Proposal**

- Define shared sentinel errors in the extractor layer:

  ```go
  var ErrNoTerminalTool = errors.New("no terminal tool was called")
  var ErrNoActivity     = errors.New("no tool calls or changes were made")
  var ErrInvalidResult  = errors.New("invalid tool result payload")
  ```

- Make each extractor follow the same contract:
  - Return `(value, nil)` when **semantics are satisfied**.
  - Return `ErrNoTerminalTool` when:
    - Required tools weren‚Äôt called, *or*
    - No terminal signal was produced.
  - Return `ErrNoActivity` when the LLM did literally nothing (e.g., no tools called, no edits).
  - Return `ErrInvalidResult` when a terminal tool was called but the payload is malformed.

- In `toolloop.Run`, classify `ErrInvalidResult` as `OutcomeExtractionError`. `ErrNoTerminalTool` and `ErrNoActivity` could be treated as ‚Äúnormal‚Äù (e.g., cause another iteration) or eventually mapped to `OutcomeMaxIterations` depending on where they surface.

- In agents, handle these via `Outcome.Kind` and/or checking `out.Err` when `Kind == OutcomeExtractionError` if you want finer-grained behavior.

**Benefit**

- Fewer subtle differences across agents about what ‚Äúno terminal tool‚Äù means.
- Easier to add logging/metrics (‚Äúhow often does the LLM fail to call a terminal tool?‚Äù) uniformly.

---

### 2.6 Dead code & debug prints

There are several pieces of code that are either dead, placeholder, or clearly debug:

- `Coder.loadTodoListFromState` and `joinStrings` are not used anywhere.
- `Coder.getExplorationHistory`, `getFilesExamined`, `getCurrentFindings` always return empty structures.
- `ExtractTodoCollectionResult` uses `fmt.Printf("DEBUG ...")` instead of structured logging.

Given that this is a **pre-release system** and you‚Äôre comfortable with breaking changes, the recommendation is:

- **Either** wire these into real behavior:
  - Actually load todo lists from state on coder startup.
  - Use exploration history and findings in prompts or logging.
- **Or** delete them now and reintroduce them when needed.

For debug prints:

- Replace `fmt.Printf` and other ad-hoc prints with structured logging via `logx.Logger` (`Debug` or `Info` level).
- If you want to keep noisy debug logs, gate them behind a configuration flag (or log level) rather than always printing.

**Benefit**

- Makes it much easier to reason about what the system *actually does*.
- Avoids surprising stdout output and keeps logs consistent.

---

### 2.7 Simplify ID/config duplication in Coder

The Coder agent currently keeps:

- A local `agentID`
- `agentConfig.ID`
- An embedded `BaseStateMachine` that also knows the agent ID

This duplication isn‚Äôt just cosmetic:

- Divergence between IDs can cause subtle bugs (mismatched metrics, logs, or dispatcher messages).

**Proposal**

- Choose a single source of truth for the agent ID:
  - Preferably `agent.Config.ID`, passed into `BaseStateMachine` at construction.
- Ensure all consumers (logs, effects, dispatcher calls) read the ID from one canonical place.

**Benefit**

- Eliminates a whole class of confusing bugs where IDs don‚Äôt line up between components.

---

### 2.8 Confirm production LLM interactions are tool-only

From the reviewed code:

- Architect prompts explicitly instruct LLMs to invoke tools and use required ‚Äúterminal‚Äù tools.
- Toolloop configs are always provided with a `ToolProvider` and `CheckTerminal`/`ExtractResult`, so production flows appear to be tool-only.

There are:

- Mock/test LLMs that return text directly (fine for tests).
- Fallback prompts that are plain text but still instruct the LLM to use tools.

Recommendation:

- Do a quick sweep of the codebase (outside `arch`, `coder`, `pm`) for direct `llmClient.Complete` calls (or equivalents) that bypass `toolloop`.
- Either:
  - Migrate remaining interactions to toolloop, or
  - Mark them clearly as non-production/testing paths.

This keeps your ‚Äútool-only‚Äù invariant strong and makes the behavior more predictable under unattended operation.

---

## 3. Toolloop refactor & escalation design (clean break)

This section focuses on the redesigned `toolloop.Run` API and how it supports your unattended operation + escalation strategy.

### 3.1 Goals for the refactor

- Keep `toolloop` as the **core testable engine**:
  - It runs the LLM/tool loop.
  - It tracks iterations and detects key failure modes.
- Make toolloop return a **typed `Outcome`** instead of a `(signal, result, error)` tuple.
- Let each agent decide **how to respond** to different `OutcomeKind` values:
  - Budget review vs human escalation vs fail hard.
- Prefer **human escalation over failure**, but keep both explicit and visible.
- Avoid gradual/compat shims in favor of a **clean break** that forces call sites to be updated.

### 3.2 New `Run` signature

```go
type OutcomeKind int

const (
    OutcomeSuccess OutcomeKind = iota
    OutcomeNoToolTwice
    OutcomeIterationLimit
    OutcomeMaxIterations
    OutcomeLLMError
    OutcomeExtractionError
)

type Outcome[T any] struct {
    Kind      OutcomeKind
    Signal    string
    Value     T
    Err       error
    Iteration int
}

func Run[T any](tl *ToolLoop, ctx context.Context, cfg *Config[T]) Outcome[T]
```

#### Behavior mapping inside `Run`

Conceptually:

- If `CheckTerminal` signaled and `ExtractResult` succeeded:
  - Return `OutcomeSuccess` with `Signal` and `Value`.
- If the LLM ignored tools twice (even after the ‚Äúplease use tools‚Äù reminder):
  - Return `OutcomeNoToolTwice` with an underlying `ErrNoToolUsageTwice` (or equivalent).
- If `Escalation.HardLimit` is reached:
  - Return `OutcomeIterationLimit` with `IterationLimitError` (or at least `Iteration`).
- If `MaxIterations` is exceeded without `Escalation.HardLimit` firing:
  - Return `OutcomeMaxIterations` with a descriptive error.
- If the LLM call itself fails persistently:
  - Return `OutcomeLLMError` with the underlying error.
- If `CheckTerminal` signaled but `ExtractResult` fails:
  - Return `OutcomeExtractionError`.

This provides a single, explicit classification of what happened in the loop.

### 3.3 Using `Outcome` in agents (examples)

#### 3.3.1 Coder ‚Äì coding loop with budget review

**Before** (simplified):

- Calls `toolloop.Run`.
- Checks `error` for `IterationLimitError` to transition to `BUDGET_REVIEW`.
- Treats other errors based on type/message.

**After**:

```go
out := toolloop.Run[CodingResult](loop, ctx, cfg)

switch out.Kind {
case toolloop.OutcomeSuccess:
    signal := out.Signal
    result := out.Value

    // Process result, then switch on signal:
    switch signal {
    case string(StateBudgetReview):
        return StateBudgetReview, false, nil
    case string(StateQuestion):
        return StateQuestion, false, nil
    case string(proto.StateError):
        return proto.StateError, false, logx.Errorf("coding returned error signal")
    // ...
    }

case toolloop.OutcomeIterationLimit:
    // Budget review escalation
    c.logger.Info("üìä Coding iteration limit reached (%d iterations), transitioning to BUDGET_REVIEW", out.Iteration)
    return StateBudgetReview, false, nil

case toolloop.OutcomeNoToolTwice:
    // Policy choice: treat as error or escalate via PM/Architect
    c.logger.Error("‚ùå LLM failed to use tools in coding state")
    return proto.StateError, false, logx.Wrap(out.Err, "coding LLM did not use tools")

case toolloop.OutcomeLLMError, toolloop.OutcomeMaxIterations, toolloop.OutcomeExtractionError:
    if isEmptyResponseError(out.Err) {
        req := agent.CompletionRequest{MaxTokens: 8192}
        return c.handleEmptyResponseError(sm, prompt, req, StateCoding)
    }
    return proto.StateError, false, logx.Wrap(out.Err, "toolloop execution failed")
}
```

- `OutcomeIterationLimit` is the only place that triggers budget review.
- `OutcomeNoToolTwice` is clearly visible; you can later change policy to human-escalate instead of fail.
- All `errors.As` and string-based classification disappear from call sites.

#### 3.3.2 Architect ‚Äì question/approval flows

Similar pattern:

```go
out := toolloop.Run[SubmitReplyResult](qh.driver.toolLoop, ctx, cfg)

switch out.Kind {
case toolloop.OutcomeSuccess:
    if out.Signal == "" {
        return fmt.Errorf("no terminal signal (submit_reply) received")
    }
    // Handle out.Value and signal normally

case toolloop.OutcomeIterationLimit:
    // Architect is a singleton; failure is expensive, escalation is preferable
    qh.driver.logger.Error("‚ùå Architect Q&A iteration limit reached (%d)", out.Iteration)
    // TODO: future: escalate via PM/human
    return fmt.Errorf("architect Q&A iteration limit hit: %w", out.Err)

case toolloop.OutcomeNoToolTwice:
    return fmt.Errorf("architect Q&A did not use tools: %w", out.Err)

default:
    return fmt.Errorf("failed to get LLM response for question: %w", out.Err)
}
```

Here the policy is:

- Architect failures are considered fatal today but should eventually cause human escalation (likely via PM).
- The `Outcome` split makes both failure and escalation triggers explicit.

#### 3.3.3 PM ‚Äì working loop and `chat_ask_user`

PM‚Äôs semantics around iteration limits are special: if an iteration limit is hit, it **must** have called something like `await_user` to provide a status update before hitting the limit.

```go
out := toolloop.Run[WorkingResult](loop, ctx, cfg)

switch out.Kind {
case toolloop.OutcomeSuccess:
    if err := d.processPMResult(out.Value); err != nil {
        return "", fmt.Errorf("failed to process PM result: %w", err)
    }
    return out.Signal, nil

case toolloop.OutcomeIterationLimit:
    if out.Signal == SignalAwaitUser || out.Value.AwaitUser {
        d.logger.Info("‚úÖ PM reached iteration limit but used await_user to update human")
        return SignalAwaitUser, nil
    }

    d.logger.Error("‚ùå PM reached iteration limit (%d) without calling await_user", out.Iteration)
    return "", fmt.Errorf("PM must call await_user with status update before iteration limit: %w", out.Err)

case toolloop.OutcomeNoToolTwice:
    d.logger.Error("‚ùå PM did not call any tools in working loop (id: %s)", d.pmID)
    return "", fmt.Errorf("PM working loop did not use tools: %w", out.Err)

default:
    return "", fmt.Errorf("PM toolloop execution failed: %w", out.Err)
}
```

This preserves your current business rule and makes it crystal clear in code.

### 3.4 Implementation plan (engineer-facing)

1. **Modify `toolloop.Run` to return `Outcome[T]`.**
   - Implement `OutcomeKind` and `Outcome[T]`.
   - Add internal sentinel errors (`ErrNoToolUsageTwice`, `IterationLimitError`) as needed to keep `Run` clean.

2. **Update all call sites in `arch`, `coder`, `pm`:**
   - Replace `signal, result, err := toolloop.Run(...)` with `out := toolloop.Run(...)`.
   - Replace `if err != nil` branches with `switch out.Kind`.
   - For each `OutcomeKind`, explicitly decide:
     - **Coder**: budget review vs story requeue vs fatal error.
     - **Architect**: fatal now, escalated later.
     - **PM**: `chat_ask_user` vs error.

3. **Standardize extractor error contracts (optional but recommended):**
   - Introduce `ErrNoTerminalTool`, `ErrNoActivity`, `ErrInvalidResult`.
   - Map them to `OutcomeExtractionError` or `OutcomeMaxIterations` as appropriate.

4. **Refactor runtimes and state-machine loops:**
   - Introduce `effect.AgentRuntime` and migrate agents.
   - Introduce `agent.RunStep` (or similar) to centralize state handler logic.

5. **Clean up dead code & logging:**
   - Remove or complete unused helpers (`loadTodoListFromState`, exploration history helpers).
   - Replace `fmt.Printf` and similar debug prints with `logx.Logger` calls.

---

## 4. Summary

Taken together, these changes will:

- Tighten up cross-agent consistency (runtime, state-machine loop, escalation semantics).
- Make `toolloop` behavior explicit, testable, and easier to reason about.
- Allow each agent to encode its own escalation policy in a small, readable switch statement rather than ad-hoc error handling.
- Let the compiler guide the migration via a deliberate breaking change to `toolloop.Run`.
- Clean out dead/debug code and reduce sources of subtle bugs (ID duplication, divergent error handling).

The system remains strongly aligned with your operational goals:

- Mostly unattended operation.
- Avoiding failure where possible.
- Preferring human escalation when the LLM cannot make meaningful progress using tools.

---

## 5. Implementation Review & Refinements (2025-01-21)

This section captures decisions from detailed code review and clarifications on implementation approach.

### 5.1 Prioritization & Phasing

**Immediate Scope (High Value):**
1. **Runtime Unification (¬ß2.1)** - One PR
2. **ID Unification (¬ß2.7)** - One PR
3. **Outcome[T] Refactor (¬ß2.4)** - One focused PR (~12 callsites)
4. **Extractor Sentinel Errors (¬ß2.5)** - One PR
5. **Cleanup (¬ß2.6)** - One PR

**Deferred/Optional:**
- **State Handler Centralization (¬ß2.2)** - Wait and see if patterns emerge naturally after Outcome refactor
- **Escalation Standardization (¬ß2.3)** - Focus on observability (log fields, metrics) rather than forcing unified control flow

### 5.2 Runtime Unification Details

**AgentRuntime scope:**
- Common: dispatching effects, logging, metadata (agent type, IDs)
- Agent-specific: `ReceiveMessage` overrides remain in each agent
- Rationale: Coder's `ReceiveMessage` logic is tied to its lifecycle (story state, todo list) which Architect/PM don't share

**Optional helper pattern:**
```go
func (r *AgentRuntime) WrapReceive(fn func(msg agent.Message) error) func(msg agent.Message) error {
    return func(msg agent.Message) error {
        // common logging/tracing
        return fn(msg)
    }
}
```

### 5.3 ID Unification Strategy

**Canonical source:** `agent.Config.ID`
- Pass into `BaseStateMachine` on construction
- Expose via `sm.ID()` or `runtime.ID()` method
- Remove duplicate `agentID` fields in driver structs
- Let compiler identify remaining references

### 5.4 Outcome[T] Design Decisions

#### OutcomeKind enum (loop-focused)

```go
type OutcomeKind int

const (
    OutcomeSuccess OutcomeKind = iota  // Terminal signal + successful extraction
    OutcomeNoToolTwice                 // LLM ignored tools 2x (loop gave up)
    OutcomeIterationLimit              // Escalation.HardLimit reached
    OutcomeMaxIterations               // MaxIterations without escalation
    OutcomeLLMError                    // LLM client failure
    OutcomeExtractionError             // CheckTerminal signaled but ExtractResult failed
)

type Outcome[T any] struct {
    Kind      OutcomeKind
    Signal    string  // CheckTerminal result (e.g., "PLAN_REVIEW", "TESTING")
    Value     T       // Extracted result (valid when Kind == OutcomeSuccess)
    Err       error   // Underlying error (non-nil for all non-Success kinds)
    Iteration int     // Last iteration count (1-indexed, useful for logs/metrics)
}
```

#### Key design principles

**OutcomeKind stays loop-focused:**
- "LLM didn't call ANY tools twice" ‚Üí `OutcomeNoToolTwice` (loop-level behavior)
- "LLM called tools, but no terminal tool" ‚Üí extractor layer (`ErrNoTerminalTool`)
- Don't add `OutcomeNoActivity` or `OutcomeSingleTurnFailed` - keep enum small

**Error handling:**
- `Outcome.Err` is always non-nil for non-Success kinds
- `toolloop` returns raw errors (no wrapping inside toolloop)
- Agents wrap with context at callsites: `logx.Wrap(out.Err, "coding toolloop failed")`
- Extractor errors surface as `OutcomeExtractionError` + `errors.Is()` checks for nuance

**SingleTurn mode:**
- No separate `OutcomeSingleTurnFailed` kind
- Failures map to existing kinds based on semantics:
  - No tools ‚Üí `OutcomeNoToolTwice`
  - Tools used but no terminal ‚Üí `OutcomeExtractionError` with `ErrNoTerminalTool`
  - Terminal tool but invalid result ‚Üí `OutcomeExtractionError` with `ErrInvalidResult`

#### Agent switch pattern

```go
switch out.Kind {
case toolloop.OutcomeSuccess:
    // Process result, then switch on signal
    switch out.Signal {
    case string(StateBudgetReview):
        return StateBudgetReview, false, nil
    case string(StateQuestion):
        return StateQuestion, false, nil
    case "PLAN_REVIEW":
        return StatePlanReview, false, nil
    }

case toolloop.OutcomeIterationLimit:
    // Budget review escalation for Coder
    c.logger.Info("Iteration limit reached (%d), transitioning to BUDGET_REVIEW", out.Iteration)
    return StateBudgetReview, false, nil

case toolloop.OutcomeNoToolTwice:
    // Policy decision: error vs escalate via PM/Architect
    return proto.StateError, false, logx.Wrap(out.Err, "LLM did not use tools")

case toolloop.OutcomeLLMError, toolloop.OutcomeMaxIterations, toolloop.OutcomeExtractionError:
    // Reuse existing error handling (empty response recovery, etc.)
    if isEmptyResponseError(out.Err) {
        req := agent.CompletionRequest{MaxTokens: 8192}
        return c.handleEmptyResponseError(sm, prompt, req, StatePlanning)
    }
    return proto.StateError, false, logx.Wrap(out.Err, "toolloop execution failed")
}
```

**Pattern benefits:**
- Switch on `Kind` first (loop outcome)
- Inside `OutcomeSuccess`, switch on `Signal` (state transition)
- Prevents treating error outcomes as if they had meaningful "next state" signals

### 5.5 Extractor Sentinel Errors

Define shared vocabulary in extractor layer:

```go
var (
    ErrNoTerminalTool = errors.New("no terminal tool was called")
    ErrNoActivity     = errors.New("no tool calls or changes were made")
    ErrInvalidResult  = errors.New("invalid tool result payload")
)
```

**Contract:**
- Return `(value, nil)` when semantics satisfied
- Return `ErrNoTerminalTool` when required tools weren't called
- Return `ErrNoActivity` when LLM did literally nothing
- Return `ErrInvalidResult` when terminal tool called but payload malformed

**Mapping to OutcomeKind:**
- All three ‚Üí `OutcomeExtractionError` (with `Outcome.Err` set to sentinel)
- Agents can use `errors.Is(out.Err, ErrNoActivity)` for fine-grained handling

**Nuance distinction:**
- `OutcomeNoToolTwice`: Loop-level guard (consecutiveNoToolTurns == 2)
- `ErrNoActivity`: Semantic failure at extractor level
- Both indicate "no meaningful progress", but different layers

### 5.6 State Handler Centralization (¬ß2.2) - Deferred

**Rationale for deferral:**
- This is preventative, not fixing a known bug
- Current switch statements are readable and debuggable
- Abstraction adds indirection that may not pay off

**Approach:**
- Complete Runtime and Outcome refactors first
- If repeated boilerplate emerges naturally, add small optional helper
- Don't force a framework - make it a tiny convenience if needed

### 5.7 Escalation Standardization (¬ß2.3) - Observability Focus

**Goal:** Unified observability/queryability, NOT identical code paths

**Approach:**
- Define shared vocabulary (string constants or small enum) for escalation kinds
- Use consistently in logging/metrics: `logger.Info("escalation", "kind", "budget_review", "agent", "coder", ...)`
- Standard metrics tags: `escalation_kind=budget_review`, `agent=coder`, `reason=iteration_limit`

**Avoid:**
- Heavy `Escalate()` helper that tries to be clever across all agents
- Forcing unified control flow when PM/Architect/Coder have legitimately different semantics

**Optional later:**
```go
logEscalation(logger, EscalationKindBudgetReview, metadata)
```

### 5.8 Critical Implementation Details

#### Breaking change scope
- **One focused PR** for Outcome[T] refactor
- ~12 callsites to update
- Keep other refactors separate to keep review tight
- No backward-compat shims needed (internal API, pre-release)

#### Error wrapping location
- `toolloop.Run` returns raw underlying errors in `Outcome.Err`
- Agents wrap at callsites where business context exists:
  ```go
  return proto.StateError, false, logx.Wrap(out.Err, "coding toolloop failed")
  ```

#### PM's `await_user` rule preservation
```go
case toolloop.OutcomeIterationLimit:
    if out.Signal == SignalAwaitUser || out.Value.AwaitUser {
        d.logger.Info("PM reached iteration limit but used await_user to update human")
        return SignalAwaitUser, nil
    }

    d.logger.Error("PM reached iteration limit (%d) without calling await_user", out.Iteration)
    return "", fmt.Errorf("PM must call await_user with status update before iteration limit: %w", out.Err)
```

**Enforcement:** Add unit test for this rule to prevent regression during refactor

### 5.9 Testing Strategy

**Unit tests (toolloop package):**
- Fake `ContextManager` and `ToolProvider`
- Force each condition:
  - No tools twice ‚Üí `OutcomeNoToolTwice`
  - Hard limit ‚Üí `OutcomeIterationLimit`
  - MaxIterations fallback ‚Üí `OutcomeMaxIterations`
  - Extraction error ‚Üí `OutcomeExtractionError`
- Assert `Outcome.Kind`, `Outcome.Err`, `Outcome.Signal` as expected

**Agent-facing tests:**
- **Coder planning/coding:**
  - Simulate `OutcomeIterationLimit` ‚Üí ensure state transitions to `BUDGET_REVIEW`
- **PM:**
  - Simulate `OutcomeIterationLimit` with/without `SignalAwaitUser` ‚Üí ensure "must call await_user" rule preserved
- **Architect:**
  - Simulate `OutcomeNoToolTwice` and `OutcomeIterationLimit` ‚Üí ensure treated as error/fatal

**Not needed:** Full end-to-end system tests (unit + targeted agent tests sufficient)

### 5.10 Migration Order

Execute in sequence:

1. **Runtime unification (¬ß2.1)**
   - Implement `AgentRuntime` in `pkg/effect/`
   - Switch agents over one at a time
   - Remove duplicate runtime boilerplate

2. **ID unification (¬ß2.7)**
   - Make `agent.Config.ID` canonical
   - Pass to `BaseStateMachine` on construction
   - Remove duplicate `agentID` fields
   - Add `sm.ID()` or `runtime.ID()` accessor

3. **Outcome[T] refactor (¬ß2.4)**
   - Define `Outcome[T]` struct and `OutcomeKind` enum
   - Change `toolloop.Run` signature
   - Update all ~12 callsites with new switch pattern
   - Add unit tests for each `OutcomeKind`

4. **Extractor sentinel errors (¬ß2.5)**
   - Define `ErrNoTerminalTool`, `ErrNoActivity`, `ErrInvalidResult`
   - Update extractor functions to return these
   - Wire into `OutcomeExtractionError` in toolloop
   - Add agent-level `errors.Is()` checks where needed

5. **Cleanup (¬ß2.6)**
   - Search for usage of `loadTodoListFromState`, `getExplorationHistory`, etc.
   - Remove truly dead code
   - Replace `fmt.Printf("DEBUG...")` with `logger.Debug()`
   - Move test-only code to `_test.go` files

Each phase is a separate PR for focused review.

### 5.11 Risk Mitigation

**Loss of error context:**
- Always set `Outcome.Err` when `Kind != OutcomeSuccess`
- Never wrap inside toolloop - only at callsites
- Log both: `logger.Error("toolloop failed", "kind", out.Kind, "err", out.Err)`

**Signal vs Outcome.Kind confusion:**
- Always switch on `Kind` first
- Only examine `Signal` inside `OutcomeSuccess` branch
- Prevents treating error outcomes as if they have meaningful state transitions

**Business rule preservation (e.g., PM's await_user):**
- Preserve exact logic in new switch structure
- Add unit tests to prevent regression
- Example callsites in ¬ß3.3.3 and ¬ß5.8 show literal translation

### 5.12 Example Callsite (Coder Planning)

See proposal ¬ß3.3.1 for detailed before/after example showing:
- Budget review escalation
- Empty response handling
- Signal-based state transitions
- Result extraction and processing

Representative callsite: `pkg/coder/planning.go:176-214`

---

## 6. Implementation Progress

### Phase 1: Runtime Unification (¬ß2.1) - ‚úÖ COMPLETED

**PR:** `refactor/runtime-unification`

**Status:** Implemented and tested

**Changes:**
- Fixed `BaseRuntime.ReceiveMessage` to use actual `replyCh` instead of broken local channel
- Added `replyCh` parameter to `BaseRuntime` constructor
- Updated all three agents (Architect, PM, Coder) to pass `replyCh` to `BaseRuntime`
- Removed Coder's custom `ReceiveMessage` override (no longer needed)
- Removed Coder's `coder *Coder` field from Runtime struct
- All three agents now have identical Runtime implementations

**Test Coverage:**
- **Unit tests** (`pkg/effect/baseruntime_test.go`): 11 tests covering all `ReceiveMessage` scenarios
  - Success case with correct message type
  - Wrong message type error
  - Timeout handling
  - Nil channel error
  - Closed channel error
  - Nil message error
  - SendMessage success and error cases
  - GetAgentID/GetAgentRole accessors
  - Logging methods

- **Integration tests** (`pkg/effect/integration_test.go`): 5 test scenarios
  - `AwaitQuestionEffect` end-to-end with BaseRuntime
  - `BudgetReviewEffect` end-to-end with BaseRuntime
  - Question effect timeout behavior
  - Wrong response type handling
  - All three agents (Coder, Architect, PM) can use blocking effects

**Coverage improvement:** BaseRuntime coverage increased from 4.2% to 48.7%

---

### Phase 2: ID Unification (¬ß2.7) - ‚úÖ COMPLETED

**PR:** `refactor/id-unification`

**Status:** Implemented and tested

**Changes:**
- Removed duplicate `agentID` field from `Coder` struct
- Removed duplicate `architectID` field from `Architect` struct
- Removed duplicate `pmID` field from `PM` struct
- All agents now use `BaseStateMachine.GetAgentID()` universally
- Updated all callsites across all three agents:
  - Coder: `driver.go`, `planning.go`, `plan_review.go`, `coding.go`, `driver_simple_test.go`
  - Architect: `driver.go`, `dispatching.go`, `escalated.go`, `questions.go`, `request.go`, `request_merge.go`, `request_spec.go`
  - PM: `driver.go`, `await_user.go`, `effects.go`, `working.go`
- Removed unused `agentConfig` and `agentCtx` variables from Coder
- Removed unused "log" import from coder driver

**Key Design Decision:**
- User insight: "why special case coder? why not get rid of the shadow and just use c.GetAgentID() universally?"
- Result: No special cases - all agents follow identical pattern
- Single source of truth: `agent.Config.ID` ‚Üí `BaseStateMachine.agentID` ‚Üí `GetAgentID()` accessor

**Test Coverage:**
- All existing tests pass (cached results)
- Full build with linting passes
- No compilation errors remain

**Files modified:**
- **Coder:**
  - `pkg/coder/driver.go` - Removed `agentID` field, use `GetAgentID()` everywhere
  - `pkg/coder/planning.go` - Updated toolloop config to use `GetAgentID()`
  - `pkg/coder/plan_review.go` - Updated toolloop config to use `GetAgentID()`
  - `pkg/coder/coding.go` - Updated toolloop config to use `GetAgentID()`
  - `pkg/coder/driver_simple_test.go` - Updated test to create BaseStateMachine
- **Architect:**
  - `pkg/architect/driver.go` - Removed `architectID` field
  - `pkg/architect/dispatching.go` - Replaced `d.architectID` with `d.GetAgentID()`
  - `pkg/architect/escalated.go` - Replaced `d.architectID` with `d.GetAgentID()`
  - `pkg/architect/questions.go` - Replaced all ID references with `GetAgentID()`
  - `pkg/architect/request.go` - Replaced `d.architectID` with `d.GetAgentID()`
  - `pkg/architect/request_merge.go` - Replaced `d.architectID` with `d.GetAgentID()`
  - `pkg/architect/request_spec.go` - Replaced `d.architectID` with `d.GetAgentID()`
  - `pkg/architect/effects.go` - Updated Runtime to use `GetAgentID()`
- **PM:**
  - `pkg/pm/driver.go` - Removed `pmID` field
  - `pkg/pm/await_user.go` - Replaced `d.pmID` with `d.GetAgentID()`
  - `pkg/pm/effects.go` - Replaced `d.pmID` with `d.GetAgentID()`
  - `pkg/pm/working.go` - Replaced all `d.pmID` references with `d.GetAgentID()`

**Outcomes:**
- ‚úÖ Zero duplicate ID fields across all agents
- ‚úÖ Single source of truth: `BaseStateMachine.GetAgentID()` used universally
- ‚úÖ No special cases - all agents follow identical pattern
- ‚úÖ Cleaner code with less shadowing and redundancy
- ‚úÖ All tests pass, full build with linting succeeds

---

### Phase 3: Outcome[T] Refactor (¬ß2.4) - ‚úÖ COMPLETED

**PR:** `refactor/outcome-type`

**Status:** Implemented and tested

**Changes:**
- Added `Outcome[T]` type with `OutcomeKind` enum in `pkg/agent/toolloop/outcome.go`
- Added extractor sentinel errors in `pkg/agent/toolloop/errors.go`:
  - `ErrNoTerminalTool` - Required terminal tools weren't called
  - `ErrNoActivity` - LLM did nothing meaningful
  - `ErrInvalidResult` - Terminal tool called but payload malformed
- Changed `toolloop.Run` signature from `(signal string, result T, err error)` to `Outcome[T]`
- Updated all 12 production callsites across PM, Coder, and Architect agents
- Updated all 11 test callsites in `toolloop_test.go`

**OutcomeKind Values:**
- `OutcomeSuccess` - Loop completed with terminal signal
- `OutcomeNoToolTwice` - LLM failed to use tools twice in a row
- `OutcomeIterationLimit` - Escalation hard limit reached
- `OutcomeMaxIterations` - MaxIterations exceeded without escalation
- `OutcomeLLMError` - LLM client failed
- `OutcomeExtractionError` - CheckTerminal signaled but ExtractResult failed

**Production Callsites Updated (12 total):**
- **PM** (1): `working.go` - Iteration limit handling for await_user
- **Coder** (3):
  - `planning.go` - Budget review escalation, question handling
  - `coding.go` - Budget review escalation, testing transition
  - `plan_review.go` - Todo collection with iteration limit as failure
- **Architect** (8):
  - `questions.go` (1) - Question answering with submit_reply
  - `request.go` (3) - Auto-response fallback, iterative approval with escalation, single-turn review
  - `request_spec.go` (1) - Spec review with feedback/approval

**Test Callsites Updated:**
- `toolloop_test.go` - 11 test functions updated to use Outcome pattern

**Code Quality Improvements:**
- ‚úÖ No magic strings - using typed constants (`StateTesting`, `StateCoding`, `StatePlanReview`)
- ‚úÖ Consistent pattern across all agents: switch on `out.Kind` first, then `out.Signal`
- ‚úÖ Removed all `errors.As(err, &iterErr)` checks - outcome kind is explicit
- ‚úÖ Better observability with `Iteration` field in all outcomes
- ‚úÖ `IterationLimitError` preserved in `out.Err` for backwards compatibility

**Example Pattern (from Coder planning.go):**
```go
out := toolloop.Run(loop, ctx, cfg)

switch out.Kind {
case toolloop.OutcomeSuccess:
    // Process result
    if err := c.processPlanningResult(sm, &out.Value); err != nil {
        return proto.StateError, false, logx.Wrap(err, "failed to process planning result")
    }

    // Handle state transitions
    switch out.Signal {
    case string(StateBudgetReview):
        return StateBudgetReview, false, nil
    case string(StateQuestion):
        return StateQuestion, false, nil
    case string(StatePlanReview):
        return StatePlanReview, false, nil
    // ...
    }

case toolloop.OutcomeIterationLimit:
    c.logger.Info("üìä Iteration limit reached (%d iterations)", out.Iteration)
    return StateBudgetReview, false, nil

case toolloop.OutcomeLLMError, toolloop.OutcomeMaxIterations, toolloop.OutcomeExtractionError:
    if c.isEmptyResponseError(out.Err) {
        return c.handleEmptyResponseError(sm, prompt, req, StatePlanning)
    }
    return proto.StateError, false, logx.Wrap(out.Err, "toolloop execution failed")

case toolloop.OutcomeNoToolTwice:
    return proto.StateError, false, logx.Wrap(out.Err, "LLM did not use tools")
}
```

**Key Design Decisions:**
- Outcome pattern eliminates all `errors.As()` checks at callsites
- OutcomeKind stays loop-focused (¬ß5.4) - doesn't include domain-specific failures
- Extractor errors (ErrNoTerminalTool) surface via OutcomeExtractionError + errors.Is
- Signal field preserved for backwards compatibility (empty string = no state transition)
- Iteration field always populated (1-indexed) for logging and metrics

**Test Results:**
- ‚úÖ All tests pass
- ‚úÖ Full build with linting succeeds
- ‚úÖ No compilation errors
- ‚úÖ All production code builds cleanly

**Files Modified (11 total):**
- **New files:**
  - `pkg/agent/toolloop/outcome.go` - Outcome[T] type and OutcomeKind enum
  - `pkg/agent/toolloop/errors.go` - Extractor sentinel errors
- **Core:**
  - `pkg/agent/toolloop/toolloop.go` - Changed Run signature to return Outcome[T]
  - `pkg/agent/toolloop/toolloop_test.go` - Updated all test callsites
- **Agents:**
  - `pkg/pm/working.go`
  - `pkg/coder/planning.go`, `pkg/coder/coding.go`, `pkg/coder/plan_review.go`
  - `pkg/architect/questions.go`, `pkg/architect/request.go`, `pkg/architect/request_spec.go`

**Outcomes:**
- ‚úÖ Breaking change forces explicit review of all toolloop usage
- ‚úÖ Type-safe result extraction with generics
- ‚úÖ Explicit classification of all loop termination conditions
- ‚úÖ No more error type checking - switch on OutcomeKind instead
- ‚úÖ Better observability and debugging with structured outcomes
- ‚úÖ Foundation for Phase 4 (Extractor Sentinel Errors refinement)

**Next Phase:** Extractor Sentinel Errors (¬ß2.5) - refine extractor contracts

---

### Phase 4: Extractor Sentinel Errors (¬ß2.5) - ‚úÖ COMPLETED

**PR:** `refactor/extractor-sentinel-errors`

**Status:** Implemented and tested

**Changes:**
- Unified all extractor functions to use sentinel errors from `pkg/agent/toolloop/errors.go`
- Replaced inconsistent error messages with shared error vocabulary
- Replaced `fmt.Printf` debug statements with structured logging via `logx.Logger`
- All extractors now follow same contract for error handling

**Sentinel Error Contract:**

All extractors now follow this contract:
- Return `(value, nil)` when semantics are satisfied
- Return `toolloop.ErrNoTerminalTool` when required tools weren't called OR no terminal signal was produced
- Return `toolloop.ErrNoActivity` when LLM did literally nothing (no tools called, no edits)
- Return `toolloop.ErrInvalidResult` when a terminal tool was called but payload is malformed

**Extractors Updated (7 total):**

1. **PM Extractor** (`pkg/pm/toolloop_results.go`):
   - `ExtractPMWorkingResult` - Now returns `ErrNoTerminalTool` instead of `WorkingResult{}, nil`

2. **Coder Extractors** (`pkg/coder/toolloop_results.go`):
   - `ExtractPlanningResult` - Returns `ErrNoTerminalTool` (was generic error message)
   - `ExtractCodingResult` - Returns `ErrNoActivity` when no tools used (was generic error)
   - `ExtractTodoCollectionResult` - Returns `ErrNoTerminalTool` (was verbose error message)
   - Replaced `fmt.Printf("DEBUG...")` with `logger.Debug()` structured logging

3. **Architect Extractors** (`pkg/architect/toolloop_results.go`):
   - `ExtractSubmitReply` - Returns `ErrNoTerminalTool` when submit_reply not called, `ErrInvalidResult` when response empty
   - `ExtractSpecReview` - Returns `ErrNoTerminalTool` (was generic error), `ErrInvalidResult` for malformed feedback
   - `ExtractReviewComplete` - Returns `ErrNoTerminalTool` (was generic error)

**Before/After Example (ExtractSubmitReply):**

```go
// BEFORE:
return SubmitReplyResult{}, fmt.Errorf("submit_reply tool was not called")
return SubmitReplyResult{}, fmt.Errorf("submit_reply called without valid response parameter")

// AFTER:
return SubmitReplyResult{}, toolloop.ErrNoTerminalTool
return SubmitReplyResult{}, toolloop.ErrInvalidResult
```

**Logging Improvements:**

Replaced debug prints with structured logging:
```go
// BEFORE:
fmt.Printf("DEBUG ExtractTodoCollectionResult: received %d results\n", len(results))
fmt.Printf("DEBUG result[%d] type=%T value=%+v\n", i, r, r)

// AFTER:
logger := logx.NewLogger("coder.extractor")
logger.Debug("ExtractTodoCollectionResult: received %d results", len(results))
logger.Debug("result[%d] type=%T value=%+v", i, r, r)
```

**Impact on OutcomeExtractionError:**

These sentinel errors now flow through `toolloop.Run` and surface as `OutcomeExtractionError`. Agents can check for specific extractor failures:

```go
case toolloop.OutcomeExtractionError:
    if errors.Is(out.Err, toolloop.ErrNoTerminalTool) {
        // Handle missing terminal tool specifically
    }
    if errors.Is(out.Err, toolloop.ErrNoActivity) {
        // Handle no activity specifically
    }
    if errors.Is(out.Err, toolloop.ErrInvalidResult) {
        // Handle malformed payload specifically
    }
```

**Benefits:**

1. **Consistent Error Semantics:** All extractors use the same vocabulary for failures
2. **Better Debugging:** Structured logging with logger levels instead of raw prints
3. **Fine-Grained Handling:** Agents can distinguish between different extraction failures via `errors.Is()`
4. **Easier Metrics:** Uniform errors make it trivial to track "how often does LLM fail to call terminal tool?"
5. **No More Ad-Hoc Messages:** Shared sentinel errors eliminate divergent error handling across agents

**Test Results:**
- ‚úÖ All tests pass (no test changes required - sentinel errors are wrapped in OutcomeExtractionError)
- ‚úÖ Full build with linting succeeds
- ‚úÖ No behavioral changes - just improved error classification

**Files Modified (3 total):**
- `pkg/pm/toolloop_results.go` - 1 extractor updated
- `pkg/coder/toolloop_results.go` - 3 extractors updated + logging improvements
- `pkg/architect/toolloop_results.go` - 3 extractors updated

**Design Decisions:**

- **No changes to toolloop.go:** Sentinel errors flow through existing `OutcomeExtractionError` path
- **No changes to agent callsites:** Agents already handle `OutcomeExtractionError`, sentinel errors just add granularity
- **Backwards compatible:** Existing error handling continues to work, `errors.Is()` checks are optional
- **Structured logging:** Debug output now respects log levels and can be filtered/disabled

**Outcomes:**
- ‚úÖ Unified error vocabulary across all extractors
- ‚úÖ Removed all `fmt.Printf` debug statements
- ‚úÖ Improved observability with structured logging
- ‚úÖ Foundation for metrics/monitoring (easy to count sentinel error occurrences)
- ‚úÖ Cleaner extractor contracts - clear separation of error cases

**Next Phase:** Cleanup (¬ß2.6) - remove dead code and finalize refactor

---

### Phase 5: Cleanup (¬ß2.6) - ‚úÖ COMPLETED

**PR:** `refactor/cleanup-dead-code`

**Status:** Implemented and tested

**Changes:**
- Removed dead code identified in the proposal (¬ß2.6)
- Removed unused functions and constants across PM, Coder, and Architect
- Fixed incorrect `//nolint:unused` comments

**Dead Code Removed:**

1. **Coder Package** (`pkg/coder/`):
   - ‚ùå Removed `loadTodoListFromState` - Never called, marked with `//nolint:unused`
   - ‚ùå Removed `getExplorationHistory()` - Always returned empty `[]string{}`
   - ‚ùå Removed `getFilesExamined()` - Always returned empty `[]string{}`
   - ‚ùå Removed `getCurrentFindings()` - Always returned empty `map[string]any{}`
   - ‚ùå Removed test `TestCoderHelperFunctions` - Only tested the above placeholder functions
   - ‚úÖ Kept `joinStrings` - Actually used by `getTodoListStatus` (removed incorrect nolint comment)
   - ‚úÖ Fixed `getTodoListStatus` - Removed incorrect `//nolint:unused` comment (function is actually used in coding.go)

2. **PM Package** (`pkg/pm/`):
   - ‚ùå Removed `renderWorkingPrompt()` - Never called, 43 lines of dead template rendering code

3. **Architect Package** (`pkg/architect/`):
   - ‚ùå Removed `acceptanceCriteriaHeader` constant - Never referenced

**Impact:**

- **Lines removed:** ~70 lines of truly dead code
- **Files modified:** 4 files
  - `pkg/coder/planning.go` - Removed 3 placeholder helper functions
  - `pkg/coder/todo_handlers.go` - Removed unused function, fixed nolint comments
  - `pkg/coder/driver_simple_test.go` - Removed test for dead helpers
  - `pkg/pm/working.go` - Removed unused rendering function
  - `pkg/architect/driver.go` - Removed unused constant

**Design Decisions:**

1. **Conservative Approach:** Only removed code explicitly identified in proposal + obvious unused items found during scan
2. **Preserved Active Code:** Kept `joinStrings` and `getTodoListStatus` despite incorrect nolint markers because they ARE used
3. **Test Cleanup:** Removed legacy test that only tested placeholder functions
4. **No Functional Changes:** All removals are pure dead code - no behavioral changes

**Why These Were Dead:**

- **Exploration Helpers:** Placeholders for future feature that was never implemented. Always returned empty structures.
- **loadTodoListFromState:** Intended for state restoration but never wired into initialization flow.
- **renderWorkingPrompt:** Template rendering function that was superseded by toolloop-based PM working flow.
- **acceptanceCriteriaHeader:** Leftover constant from refactoring, no longer used in any templates.

**Test Results:**
- ‚úÖ All tests pass
- ‚úÖ Full build with linting succeeds
- ‚úÖ No compilation errors or warnings
- ‚úÖ No behavioral changes

**Benefits:**

1. **Reduced Codebase Size:** ~70 lines of dead code eliminated
2. **Clearer Intent:** Removed misleading nolint comments that suggested code would be used "next"
3. **Easier Maintenance:** Fewer functions to maintain and reason about
4. **Better Code Quality:** Removes confusion about what code is actually active vs placeholder

**Files Modified (4 total):**
- `pkg/coder/planning.go` - Removed 3 placeholder helpers
- `pkg/coder/todo_handlers.go` - Removed unused function + fixed comments
- `pkg/coder/driver_simple_test.go` - Removed obsolete test
- `pkg/pm/working.go` - Removed unused rendering function
- `pkg/architect/driver.go` - Removed unused constant

**Outcomes:**
- ‚úÖ All dead code from proposal removed
- ‚úÖ Incorrect nolint comments fixed
- ‚úÖ Codebase is cleaner and more maintainable
- ‚úÖ No functional regressions
- ‚úÖ Foundation for future feature work without misleading placeholders

**Next Steps:** Phase 6 - Budget Review Fixes

---

### Phase 6: Budget Review Fixes - ‚úÖ COMPLETED

**Branch:** `bugfix-2`

**Status:** Implemented and tested

**Issues Identified from Production Logs:**

**Issue 1: Missing Context in Architect Budget Review**
- **Problem**: Architect complained "I do not have any recent tool calls, file diffs, shell logs..." when receiving budget review requests
- **Root Cause**: Architect's `generateBudgetPrompt()` was trying to extract metadata and re-render templates, but the coder already rendered all context into the `Content` field via `getBudgetReviewContent()`
- **Solution**: Simplified `pkg/architect/request_budget.go` to just extract and return `approvalPayload.Content` instead of trying to re-render from metadata
- **Before**: ~130 lines of metadata extraction, template rendering, story queue lookups
- **After**: ~25 lines that simply return the pre-rendered Content field
- **Files Changed**: `pkg/architect/request_budget.go`

**Issue 2: Incorrect State Transition After Budget Review**
- **Problem**: When CODING‚ÜíBUDGET_REVIEW with NEEDS_CHANGES response, coder was transitioning to PLANNING instead of returning to CODING
- **Log Evidence**: "Budget review needs changes from , pivoting to PLANNING" (note empty string after "from")
- **Root Cause**: `originStr` was empty ("") when processing budget review result, causing the comparison `if originStr == string(StateCoding)` to fail and fall through to the else branch that returns to PLANNING
- **Solution**: Added diagnostic logging at three key points to trace origin state flow:
  1. `pkg/coder/driver.go:705` - Log when origin is stored via `checkLoopBudget`
  2. `pkg/coder/coding.go:350` - Log when origin is stored via empty response path
  3. `pkg/coder/budget_review.go:70` - Log when origin is retrieved and processed
- **Files Changed**: `pkg/coder/budget_review.go`, `pkg/coder/driver.go`, `pkg/coder/coding.go`

**Key Insights:**

- **Payload.Content vs Metadata**: Content field contains core context (fully rendered templates), Metadata is for debugging/metrics only
- **Template Rendering**: Coder's `getBudgetReviewContent()` already renders `BudgetReviewRequestCodingTemplate`/`PlanningTemplate` with all necessary context (RecentActivity, IssuePattern, etc.)
- **State Persistence**: Origin state is stored in state machine data with `KeyOrigin = "origin"` to track whether review came from CODING or PLANNING

**Testing Plan:**

1. Trigger budget review from CODING state (exceed 8 iterations)
2. Verify architect receives full context in budget review prompt
3. Verify NEEDS_CHANGES response returns coder to CODING (not PLANNING)
4. Check diagnostic logs to confirm origin state is set and retrieved correctly

**Budget Review Flow:**

1. Coder exceeds iteration budget (8 iterations)
2. Creates `BudgetReviewEffect` with rendered content
3. Transitions to BUDGET_REVIEW state
4. Sends REQUEST to architect with `ApprovalRequestPayload`
5. Architect receives fully rendered context in `Content` field
6. Returns RESPONSE with status (APPROVED/NEEDS_CHANGES/REJECTED)
7. Coder processes result and returns to origin state (CODING or PLANNING)

**Files Modified (4 total):**

- `pkg/architect/request_budget.go` - Simplified to use Content field
- `pkg/coder/budget_review.go` - Added origin state diagnostic logging
- `pkg/coder/driver.go` - Added origin state diagnostic logging
- `pkg/coder/coding.go` - Added origin state diagnostic logging (empty response path)

**Code Quality Improvements:**

- ‚úÖ Eliminated duplicate template rendering in architect
- ‚úÖ Clear separation: coder renders context, architect consumes it
- ‚úÖ Added diagnostic logging to trace state persistence issues
- ‚úÖ No behavioral changes beyond fixes

**Test Results:**

- ‚úÖ Code compiles cleanly with `make build`
- ‚úÖ All linting passes
- ‚úÖ No compilation errors or warnings

**Benefits:**

1. **Issue 1 - Fixed**: Architect now receives full context (recent activity, issue patterns) in budget review requests
2. **Issue 2 - Diagnosable**: Logging added to trace origin state flow and identify why it's empty
3. **Cleaner Architecture**: Coder is responsible for rendering context, architect just consumes it
4. **Better Observability**: Diagnostic logs make state persistence issues visible

**Outcomes:**

- ‚úÖ Architect budget review requests now include full context
- ‚úÖ Diagnostic logging added to identify origin state persistence issues
- ‚úÖ Cleaner separation of concerns (rendering vs consuming)
- ‚úÖ Foundation for resolving state transition bug

**Next Steps:** Test fixes in production to verify both issues are resolved
